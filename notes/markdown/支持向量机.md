- 一种二类分类模型
# 1 线性可分支持向量机与硬间隔最大化
## 1.1 线性可分支持向量机
假设给定一个特征空间上的训练数据集$T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$。其中，$x_i\in \chi=\mathbb{R}^n$，$y_i\in \gamma=\{+1,-1\}$，$i=1,2,...,N$。

> 学习目标是在特征空间上找到一个分离超平面。分离超平面的方程$w\cdot x+b=0$。

其相应的分类决策函数为$$\huge f(x)=\text{sign}(w^*\cdot x+b^*)$$
称为线性可分支持向量机。

## 1.2 函数间隔和几何间隔
函数间隔就是利用类标记符号$y$和实例点$x$与$w\cdot x+b=0$之间的距离来判定分类的正确性和确信度。$$\huge \hat\gamma_i = y_i(w\cdot x_i+b)$$
虽然间隔函数能够表示分类预测的正确性和确信度。但是当模型参数变成$2w$和$2b$时，得到的超平面没有变化但函数间隔是原来的两倍了。

因此，我们应该对分离超平面的$w$进行约束，例如规范化（$||w||=1$），此时的间隔函数会变成几何函数。$$\huge \gamma_i=y_i({{w}\over{||w||}}\cdot x+{{b}\over{||w||}})$$
其中，$||W||$为$w$的$L_2$范数。

## 1.3 间隔最大化
支持向量机学习的基本想法就是
1. 能够正确划分训练数据集
2. 几何间隔最大分离超平面

### 壹、最大间隔分离超平面
从几何间隔能发现，最大化的$\huge{{\hat \gamma}\over{||w||}}$可以找到最优模型，因此$$\huge \max_{w,b}{{\hat \gamma}\over{||w||}}$$$$\huge s.t. \quad y_i(w\cdot x_i+b)\geq\hat\gamma, \quad i=1,2,...,N$$
而其中取$\hat\gamma=1$代入上面的最优化问题，可以注意到最大化$\huge {{1}\over{||w||}}$和最小化$\huge {1\over2}||w||^2$是等价的，于是得到下面的线性可分支持向量机学习的最优化问题。$$\huge \min_{w,b}{1\over2}||w||^2$$$$\huge s.t.\quad y_i(w\cdot x_i+b)-1\geq0,\quad i=1,2,...,N$$
而这个就是一个凸二次规划问题$$\huge \begin{split}&\min_w f(w)\\
&s.t.\quad{\begin{split}&g_i(w)\leq0,\quad i=1,2,...,k\\&h_i(w)=0,\quad i=1,2,...,l\end{split}}\end{split}$$
其中，目标函数$f(w)$和约束函数$g_i(w)$都是$\mathbb{R}^n$上的连续可微分的凸函数，约束函数$h_i(w)$是$\mathbb{R}^n$上的仿射函数。

> 仿射函数就是在保持原有函数的特征的情况下进行的线性变换，但是仿射函数是一个1阶函数（其中是由矩阵和向量组成的函数）

1. [什么是仿射函数？](https://blog.csdn.net/houhuipeng/article/details/92836041)
2. [仿射函数、线性函数的区别？](https://www.zhihu.com/question/52571431)
3. [策略算法工程师之路-凸二次优化](https://zhuanlan.zhihu.com/p/100041443)
### 贰、最大间隔分离超平面的存在唯一性
> 若训练数据集$T$线性可分，则可将训练数据集的样本点完全正确分开的最大间隔分离超平面==存在==且==唯一==
#### （1）存在性
上述已证明

#### （2）唯一性
1. 首先，证明$w^*$的唯一性。
假设问题拥有两个解，分别为$(w_1^*,b_1^*)$和$(w_2^*,b_2^*)$。
显然$||w_1^*||=||w_2^*||=c$，其中$c$是一个常数。令$\huge w = {{w_1^*+w_2^*}\over{2}}$，$\huge b = {{b_1^*+b_2^*}\over{2}}$。从而有，$$\huge c\leq ||w||\leq{1\over 2}||w_1^*||+{1\over 2}||w_2^*||=c$$
上述表达式中的不等号能变成等号，即为$\huge ||w||={1\over 2}||w_1^*||+{1\over 2}||w_2^*||$，从而有$w_1^* = \lambda w_2^*$，$|\lambda| = 1$。若$\lambda = -1$，则$w=0$，解便不存在。因此必有$\lambda =1$，即$$\huge w_1^*=w_2^*$$
2. 其次，证明$b*$的唯一性
设$x_1'$和$x_2'$是集合$\{x_i|y_i=+1\}$和$x_1''$和$x_2''$是集合$\{x_i|y_i=-1\}$。
则由$\huge b_1^*=-{1\over 2}(w^*\cdot x_1'+w^*\cdot x_1'')$, $\huge b_2^*=-{1\over 2}(w^*\cdot x_2'+w^*\cdot x_2'')$，得$$\huge b_1^*-b_2^* = -{1\over 2}[w^*\cdot(x_1'-x_2')+w^*\cdot(x_2''-x_2'')]$$
又因为$$\huge w^*\cdot x_2'+b_1^*\geq 1=w^*\cdot x_1'+b_1'$$$$\huge w^*\cdot x_1'+b_2^*\geq 1=w^*\cdot x_2'+b_2'$$
所以，$\huge w^*\cdot (x_1'-x_2')=0$。同理有$w^*\cdot (x_1''-x_2'')=0$。因此，$$\huge b_1^*-b_2^*=0$$
由此可以得出$w$和$b$都是唯一的，解的唯一性得证

### 叁、支持向量和间隔边界
在线性可分情况下，训练数据集的样本中与分离超平面距离最近的样本的点实例称为支持向量。支持向量是使约束条件式等号成立的点，即：$$\huge y_i(w\cdot x_i+b)-1=0$$
对$y_i=+1$的正例点，支持向量在超平面$$\huge H_1=w\cdot x+b=1$$
对$y_i=-1$的正例点，支持向量在超平面$$\huge H_2=w\cdot x+b=-1$$
![[Pasted image 20230810144808.png|center]]

## 1.4 学习的对偶算法
为了求解线性可分支持向量机的求优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解。
这样做的优点有：
1. 对偶问题更容易求解
2. 自然引入核函数，进而推广到非线性分类问题

首先，构建拉格朗日函数。为此，对每一个不等式约束引入拉格朗日函数乘子$\alpha_i\geq0, i=1,2,...,N$，得到拉格朗日函数：$$\huge L(w,b,\alpha)={1\over2}||w||^2-\sum^N_{i=1}\alpha_iy_i(w\cdot x_i+b)+\sum^N_{i=1}\alpha_i$$
其中，$\alpha=(\alpha_1,\alpha_2,...,\alpha_N)^T$为拉格朗日乘子向量。

此时的最优化问题就变成了：$$\huge \max_\alpha\min_{w,b}L(w,b,\alpha)$$

分开解这个最优化问题：
（1）求$\min_{w,b}L(w,b,\alpha)$
将拉格朗日函数对$w,b$进行偏导并令其等于$0$
$$\huge \begin{align}
&\nabla_wL(w,b,\alpha) = w - \sum^N_{i=1}\alpha_iy_ix_i=0\\
&\nabla_bL(w,b,\alpha) = -\sum^N_{i=1}\alpha_i
y_i=0\end{align}$$
得$$\huge 
\begin{align}
& w = \sum^N_{i=1} \alpha_i y_i x_i \\
&\sum^N_{i=1} \alpha_iy_i = 0
\end{align}$$
带入回原拉格朗日函数，可得：$$\huge \begin{align}
L(w,b,\alpha) &={1\over2}\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum^N_{i=1}\alpha_iy_i((\sum^N_{j=1}\alpha_jy_jx_j)\cdot x_i+b) + \sum^N_{i=1}\alpha_i\\
&=-{1\over2}\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) + \sum^N_{i=1}\alpha_i
\end{align}$$
即$$\huge \min_{w,b}L(w,b,\alpha)=-{1\over2}\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) + \sum^N_{i=1}\alpha_i$$
（2）求$\max_\alpha(\min_{w,b} L(w,b,\alpha))$
$$\huge \begin{align}
&\max_\alpha \quad-{1\over2}\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) + \sum^N_{i=1}\alpha_i\\
&\begin{split}\text{s.t.}\quad&\sum^N_{i=1}\alpha_iy_i=0\\
&\alpha_i\geq0,\quad i=1,2,...,N
\end{split}\end{align}$$

转换成等价的对偶最优化问题：$$\huge \begin{align}
&\min_\alpha \quad{1\over2}\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum^N_{i=1}\alpha_i\\
&\begin{split}\text{s.t.}\quad&\sum^N_{i=1}\alpha_iy_i=0\\
&\alpha_i\geq0,\quad i=1,2,...,N
\end{split}\end{align}$$
###  定理1.2
设$\alpha^* = (\alpha^*_1,\alpha^*_2,...,\alpha^*_l)^T$ 是对偶最优化问题的的解，则存在下标$j$，使得$\alpha^*_j>0$，并可按下式求得原始最优化问题的解$w^*,b^*$:$$\huge w^* = \sum^N_{i=1}\alpha^*_iy_ix_i$$
$$\huge b^*=y_j-\sum^N_{i=1}\alpha^*_iy_i(x_i\cdot x_j)$$
从支持向量机的原始公式能得出，$$\huge y_j(w^*\cdot x_j+b^*)-1=0$$
将$w^*$带入以上式子可以注意到$y^2_j=1$，即得分离超平面$$\huge \sum^N_{i=1}\alpha^*_iy_i(x\cdot x_i)+b^*=0$$
分类决策函数可以写成$$\huge f(x) = \text{sign}(\sum^N_{i=1}\alpha^*_iy_i(x\cdot x_i)+b^*)$$
这说明了分类决策函数只依赖于输入$x$和训练样本输入的内积。这个式子也称为线性可分支持向量机的对偶形式。

# 2 线性支持向量机与软间隔最大化
## 2.1 线性支持向量机
线性可分问题的支持向量机学习方法对于线性不可分训练数据是不适用的。为了解决这个问题需要改硬间隔最大化，使其成为软间隔最大化。

> 线性可分是数据点可以被一个超平面完全分离
> 线性不可分是数据点无法别一个超平面分离。
> 例子：XOR问题就是典型的线性不可分例子

假设给定一个特征空间上的训练数据集$$\huge T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$$其中，$x_i\in\mathcal{X}=\mathbb{R}^n$, $y_i\in\mathcal{y}=\{+1,-1\}$, $i=1,2,...,N$, $x_i$为第$i$个特征向量, $y_i$为$x_i$的类标记。再假设训练数据集是线性不可分的
通常情况下是，训练数据集中有一些特异点，将这些特异点除去后，剩下大部分的样本点组成的集合是线性可分的。

线性不可分意味着某些样本点$(x_i,y_i)$不能满足函数间隔大于等于$1$的约束条件。为解决这个问题，可以对每个样本点$(x_i,y_i)$引进一个松弛变量 $\xi_i\geq0$，使间隔函数加上松弛变量大于等于 $1$。这样玉树条件变为$$\huge y_i(w\cdot x_i+b)\geq1-\xi_i$$
同时目标函数由原来的${1\over2}\|w\|^2$变成了$$\huge {1\over2}\|w\|^2+C\sum^N_{i=1}\xi_i$$这里，$C>0$称为惩罚函数，一般有应用问题决定，$C$ 值大时对五分类的惩罚增大，反之惩罚减少。 

为了得到最优解，线性不可分的线性支持向量问题的虚席问题编程如下凸二次规划问题：$$\huge \begin{align}
& \min_{w,b,\xi}{1\over2}\|w\|^2 + C\sum^N_{i=1}\xi_i \\
&\begin{split}\text{s.t.}\quad & y_i(w\cdot x_i+b)\geq1-\xi_i,\quad i=1,2,...,N\\
& \xi_i\geq0,\quad i=1,2,...,N
\end{split}\end{align}$$
## 2.2 学习的对偶算法
原始问题的对偶形式是$$\huge \begin{align}
& \min_{\alpha}{1\over2}\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)-\sum^N_{i=1}\alpha_i \\
&\begin{split}\text{s.t.}\quad & \sum^N_{i=1}\alpha_iy_i=0\\
& 0\leq\alpha_i\leq C,\quad i=1,2,...,N
\end{split}\end{align}$$
原始最优化问题的拉格朗日函数是$$\huge L(w,b,\xi,\alpha,\mu)\equiv{1\over2}\|w\|^2+C\sum^N_{i=1}\xi_i-\sum^N_{i=1}\alpha_i(y_i(w\cdot x_i+b)-1+\xi_i)-\sum^N_{i=1}\mu_i\xi_i$$
其中，$\alpha_i\geq0, \mu_i\geq0$。

对偶问题时拉格朗日函数的极大极小问题。首先求$L(w,b,\xi,\alpha,\mu)$对$w,b,\xi$ 的极小，由$$\huge \begin{align}
&\nabla_wL(w,b,\xi,\alpha,\mu) = w - \sum^N_{i=1}\alpha_iy_ix_i=0\\
&\nabla_bL(w,b,\xi,\alpha,\mu) = - \sum^N_{i=1}\alpha_iy_i=0\\
&\nabla_{\xi_i}L(w,b,\xi,\alpha,\mu) = C - \alpha_i-\mu_i=0\\
\end{align}$$得$$\huge \begin{align}
&w = \sum^N_{i=1}\alpha_iy_ix_i\\
&\sum^N_{i=1}\alpha_iy_i=0\\
&C-\alpha_i-\mu_i=0
\end{align}$$
带入原始最优化问题的拉格朗日函数得$$\huge \min_{w,b,\xi}L(w,b,\xi,\alpha,\mu)=-{1\over2}\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum^N_{i=1}\alpha_i$$再对$\min_{w,b,\xi}L(w,b,\xi,\alpha,\mu)$求$\alpha$的极大，即对偶问题：$$\huge \begin{align}&\max_\alpha-{1\over2}\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j)+\sum^N_{i=1}\alpha_i\\
&\begin{split}\text{s.t.} &\sum^N_{i=1}\alpha_iy_i=0\\
&C - \alpha_i-\mu_i=0\\
&\alpha_i\geq0\\
&\mu_i\geq0,\quad i=1,2,...,N
\end{split}\end{align}$$从中消去$\mu_i$，从而只留下变量$\alpha_i$，并将约束写成$$\huge 0\leq\alpha_i\leq C$$再将对目标函数求极大转换为求极小，于是得到对偶问题。

## 2.3 支持向量
![[Pasted image 20230915174057.png|center]]

## 2.4 合页损失函数
对于线性支持向量机学习来说，其模型为分离超平面$w^*\cdot x + b^*=0$及决策函数$f(x)=\text{sign}(w^*\cdot + b^*)$，其学习策略为软间隔最大化，学习算法为凸二次规划。

线性支持向量机学习还有另一个解释，就是最小化以下目标函数：$$\huge \sum^N_{i=1}[1-y_i(w\cdot x_i+b)]_++\lambda\|w\|^2$$
目标函数的第1项是经验风险，函数$$\huge L(y(w\cdot x+b))=[1-y(w\cdot x+b)]$$称为合页损失函数（Hinge Loss Function）。下标“+”表示以下正值的函数。$$\huge [z]_+=\begin{cases}z,\quad z>0\\0,\quad z\leq0\end{cases}$$
### 定理1.4
线性支持向量机原始最优化问题：$$\huge \begin{align}
&\min_{w,b,\xi}\quad {1\over2}\|w\|^2+C\sum^N_{i=1}\xi_i\\
&\begin{split}\text{s.t.}\quad &y_i(w\cdot x_i+b)\geq1-\xi_i,\quad i=1,2,...,N\\
& \xi_i \geq0,\quad i=1,2,...,N
\end{split}\end{align}$$
其等价最优化问题为$$\huge \min_{w,b}\sum^N_{i=1}[1-y_i(w\cdot x_i+b)]_++\lambda\|w\|^2$$
# 3 非线性支持向量机与核函数
在处理线性分类问题，线性分类支持向量机是一种非常有效的方法。但是，在面对到非线性数据集的时候就会分棘手，这是可以使用非线性支持向量机。

这里叙述的非线性支持向量机的特点是利用了==核函数==。

## 3.1 核函数
### 1. 非线性分类问题
非线性分类问题是指通过利用非线性模型才能很好地进行分类的问题

假设给定一个训练数据集$T = \{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$，其中实例$x_i$属于输入空间，$s_i\in\mathcal{X}=\mathbf{R}^n$，对应的标记有两类$y_i\in\mathcal{Y}=\{-1,+1\}, i=1,2,...,N$。如果能用$\mathbf{R}^n$中的一个超曲面将正负例正确分开，则称这个问题为非线性可分问题。

非线性问题往往不好求解，所以希望能用线性分类问题的方法解决这个问题。因此采取的方法是进行一个非线性变换，将非线性问题变成线性问题。

设原空间为$\mathcal{X}\subset \mathbf{R}^2$， $x=(x^{(1)},x^{(2)})^T\in\mathcal{X}$，新空间为$\mathcal{Z}\subset\mathbf{R}^2$， $z=(z^{(1)},z^{(2)})^T\in\mathcal{Z}$， 定义从元空间到新空间的变换（映射）：$$\huge z=\phi(x)=((x^{(1)})^2,(x^{(1)})^2)^T$$
经过变换$z=\phi(x)$，原空间$\mathcal{X}\subset \mathbf{R}^2$变换为新空间$\mathcal{Z}\subset\mathbf{R}^2$，由$$\huge w_1(x^{(1)})^2+w_2(x^{(2)})^2+b=0$$变换成了新空间的直线$$\huge w_1z^{(1)}+w_2z^{(2)}+b=0$$

### 2. 核函数的定义
设 $\mathcal{X}$ 是输入空间，又设 $\mathcal{H}$ 为特征空间，如果存在一个从 $\mathcal{X}$ 到 $\mathcal{H}$ 的映射$$\huge \phi(x):\mathcal{X}\rightarrow\mathcal{H}$$使得对所有$x,z\in\mathcal{X}$，函数$K(x,z)$满足条件$$\huge K(x,z)=\phi(x)\cdot\phi(z)$$
则称$K(x,z)$为核函数，$\phi(x)$为映射函数，式中$\phi(x)\cdot\phi(z)$为$\phi(x)$和$\phi(z)$的内积

### 3. 核技巧在支持向量机中的应用
我们可以注意到线性支持向量机的对偶问题中，无论是目标函数还是决策函数都只涉及输入实例与实例之间的内积。

在对偶问题的目标函数中的内积$x_i\cdot x_j$可以用核函数$K(x_i,x_j)=\phi(x_i)\cdot\phi(x_j)$来代替。此时对偶问题的目标函数成为$$\huge W(\alpha)={1\over2}\sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_jK(x_i,x_j)-\sum^N_{i=1}\alpha_i$$同样，分类的决策函数中的内积也能使用核函数进行替代，成为：$$\huge \begin{align} f(x) 
&= \text{sign}(\sum^{N_s}_{i=1}\alpha_i^*y_i\phi(x_i)\cdot\phi(x)+b^*)\\
&= \text{sign}(\sum^{N_s}_{i=1}\alpha_i^*y_iK(x_i,x)+b^*)
\end{align}$$
这等价于经过映射函数 $\phi$ 将原来的输入空间换到一个新的特征空间，将输入空间中的内积换成特征空间中内积。

也就是说，在核函数$K(x,z)$给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐式地在特征空间进行的，不需要显式地定义特征空间和映射函数，这就成为核技巧。

## 3.2 正定核
假设 $K(x,z)$ 是定义在 $\mathcal{X}\times\mathcal{X}$ 上的对称函数，并且对任意的 $x_1,x_2,...,x_m\in\mathcal{X}$，$K(x,z)$ 关于 $x_1,x_2,...,x_m$ 的Gram矩阵是半正定的。可以根据函数$K(x,z)$，构成一个希尔伯特空间，其步骤是：
### 1. 定义映射，构成向量空间 $\mathcal{S}$
先定义映射$$\huge \phi:x\rightarrow K( \cdot ,x)$$
根据这一映射，对任意$x_i\in\mathcal{X}$, $\alpha_i\in\mathbf{R}$, $i=1,2,...,m$，定义线性组合$$\huge f(\cdot)=\sum^m_{i=1}\alpha_iK(\cdot,x_i)$$
由于集合$\mathcal{S}$是由线性组合，因此其加法和乘法运算是封闭的，所以$\mathcal{S}$构成一个向量空间。

### 2. 在 $\mathcal{S}$ 上定义内积，使其成为内积空间
在 $\mathcal{S}$ 上定义一个运算 $*$：对任意 $f,g\in \mathcal{S}$，$$\huge f(\cdot)=\sum^m_{i=1}\alpha_iK(\cdot,x_i)$$$$\huge g(\cdot)=\sum^m_{j=1}\beta_iK(\cdot,z_j)$$定义运算$*$ $$\huge f*g=\sum ^m_{i=1}\sum^l_{j=1}\alpha_i\beta_lK(x_i,z_j)$$
证明... $*$ 为向量空间 $\mathcal{S}$ 的内积。因此 $\mathcal{S}$ 是一个内积空间，所以$$\huge f\cdot g=\sum ^m_{i=1}\sum^l_{j=1}\alpha_i\beta_lK(x_i,z_j)$$
### 3. 将内积空间 $\mathcal{S}$ 完备化为希尔伯特空间
现在将内积空间 $\mathcal{S}$ 完备化。由内积可得范数：$$\huge \|f\|=\sqrt{f\cdot f}$$因此，$\mathcal{S}$ 是一个赋范向量空间。根据泛函分析理论，对于不完备的赋范向量空间 $\mathcal{S}$，一定可以使之完备化，得到完备的赋范向量空间 $\mathcal{H}$。

这一希尔伯特空间 $\mathcal{H}$ 称为再生希尔伯特空间。这是由核$K$具有再生性，既满足：$$\huge K(\cdot,x)\cdot f=f(x)$$及$$\huge K(\cdot,x)\cdot K(\cdot,z)=K(x,z)$$称为再生核。

### 4. 正定核的充要条件
