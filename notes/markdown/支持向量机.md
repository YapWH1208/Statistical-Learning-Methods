- 一种二类分类模型
# 1 线性可分支持向量机与硬间隔最大化
## 1.1 线性可分支持向量机
假设给定一个特征空间上的训练数据集$T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$。其中，$x_i\in \chi=\mathbb{R}^n$，$y_i\in \gamma=\{+1,-1\}$，$i=1,2,...,N$。

> 学习目标是在特征空间上找到一个分离超平面。分离超平面的方程$w\cdot x+b=0$。

其相应的分类决策函数为$$\huge f(x)=\text{sign}(w^*\cdot x+b^*)$$
称为线性可分支持向量机。

## 1.2 函数间隔和几何间隔
函数间隔就是利用类标记符号$y$和实例点$x$与$w\cdot x+b=0$之间的距离来判定分类的正确性和确信度。$$\huge \hat\gamma_i = y_i(w\cdot x_i+b)$$
虽然间隔函数能够表示分类预测的正确性和确信度。但是当模型参数变成$2w$和$2b$时，得到的超平面没有变化但函数间隔是原来的两倍了。

因此，我们应该对分离超平面的$w$进行约束，例如规范化（$||w||=1$），此时的间隔函数会变成几何函数。$$\huge \gamma_i=y_i({{w}\over{||w||}}\cdot x+{{b}\over{||w||}})$$
其中，$||W||$为$w$的$L_2$范数。

## 1.3 间隔最大化
支持向量机学习的基本想法就是
1. 能够正确划分训练数据集
2. 几何间隔最大分离超平面

### 壹、最大间隔分离超平面
从几何间隔能发现，最大化的$\huge{{\hat \gamma}\over{||w||}}$可以找到最优模型，因此$$\huge \max_{w,b}{{\hat \gamma}\over{||w||}}$$$$\huge s.t. \quad y_i(w\cdot x_i+b)\geq\hat\gamma, \quad i=1,2,...,N$$
而其中取$\hat\gamma=1$代入上面的最优化问题，可以注意到最大化$\huge {{1}\over{||w||}}$和最小化$\huge {1\over2}||w||^2$是等价的，于是得到下面的线性可分支持向量机学习的最优化问题。$$\huge \min_{w,b}{1\over2}||w||^2$$$$\huge s.t.\quad y_i(w\cdot x_i+b)-1\geq0,\quad i=1,2,...,N$$
而这个就是一个凸二次规划问题$$\huge \begin{split}&\min_w f(w)\\
&s.t.\quad{\begin{split}&g_i(w)\leq0,\quad i=1,2,...,k\\&h_i(w)=0,\quad i=1,2,...,l\end{split}}\end{split}$$
其中，目标函数$f(w)$和约束函数$g_i(w)$都是$\mathbb{R}^n$上的连续可微分的凸函数，约束函数$h_i(w)$是$\mathbb{R}^n$上的仿射函数。

> 仿射函数就是在保持原有函数的特征的情况下进行的线性变换，但是仿射函数是一个1阶函数（其中是由矩阵和向量组成的函数）

1. [什么是仿射函数？](https://blog.csdn.net/houhuipeng/article/details/92836041)
2. [仿射函数、线性函数的区别？](https://www.zhihu.com/question/52571431)
3. [策略算法工程师之路-凸二次优化](https://zhuanlan.zhihu.com/p/100041443)
4. 
### 贰、最大间隔分离超平面的存在唯一性
> 若训练数据集$T$线性可分，则可将训练数据集的样本点完全正确分开的最大间隔分离超平面==存在==且==唯一==
#### （1）存在性
上述已证明

#### （2）唯一性
1. 首先，证明$w^*$的唯一性。
假设问题拥有两个解，分别为$(w_1^*,b_1^*)$和$(w_2^*,b_2^*)$。
显然$||w_1^*||=||w_2^*||=c$，其中$c$是一个常数。令$\huge w = {{w_1^*+w_2^*}\over{2}}$，$\huge b = {{b_1^*+b_2^*}\over{2}}$。从而有，$$\huge c\leq ||w||\leq{1\over 2}||w_1^*||+{1\over 2}||w_2^*||=c$$
上述表达式中的不等号能变成等号，即为$\huge ||w||={1\over 2}||w_1^*||+{1\over 2}||w_2^*||$，从而有$w_1^* = \lambda w_2^*$，$|\lambda| = 1$。若$\lambda = -1$，则$w=0$，解便不存在。因此必有$\lambda =1$，即$$\huge w_1^*=w_2^*$$
2. 其次，证明$b*$的唯一性
设$x_1'$和$x_2'$是集合$\{x_i|y_i=+1\}$和$x_1''$和$x_2''$是集合$\{x_i|y_i=-1\}$。
则由$\huge b_1^*=-{1\over 2}(w^*\cdot x_1'+w^*\cdot x_1'')$,$\huge b_2^*=-{1\over 2}(w^*\cdot x_2'+w^*\cdot x_2'')$，得$$\huge b_1^*-b_2^* = -{1\over 2}[w^*\cdot(x_1'-x_2')+w^*\cdot(x_2''-x_2'')]$$
又因为$$\huge w^*\cdot x_2'+b_1^*\geq 1=w^*\cdot x_1'+b_1'$$$$\huge w^*\cdot x_1'+b_2^*\geq 1=w^*\cdot x_2'+b_2'$$
所以，$\huge w^*\cdot (x_1'-x_2')=0$。同理有$w^*\cdot (x_1''-x_2'')=0$。因此，$$\huge b_1^*-b_2^*=0$$
由此可以得出$w$和$b$都是唯一的，解的唯一性得证

### 叁、支持向量和间隔边界
在线性可分情况下，训练数据集的样本中与分离超平面距离最近的样本的点实例称为支持向量。支持向量是使约束条件式等号成立的点，即：$$\huge y_i(w\cdot x_i+b)-1=0$$
对$y_i=+1$的正例点，支持向量在超平面$$\huge H_1=w\cdot x+b=1$$
对$y_i=-1$的正例点，支持向量在超平面$$\huge H_2=w\cdot x+b=-1$$
![[Pasted image 20230810144808.png|center]]

## 1.4 学习的对偶算法
